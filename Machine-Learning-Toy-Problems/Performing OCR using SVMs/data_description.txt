Background:

mage processing is a difficult task for many types of machine learning algorithms.
The relationships linking patterns of pixels to higher concepts are extremely complex
and hard to define. For instance, it's easy for a human being to recognize a face, a cat,
or the letter "A", but defining these patterns in strict rules is difficult. Furthermore,
image data is often noisy. There can be many slight variations in how the image was
captured, depending on the lighting, orientation, and positioning of the subject.

SVMs are well-suited to tackle the challenges of image data. Capable of learning
complex patterns without being overly sensitive to noise, they are able to recognize
visual patterns with a high degree of accuracy. Moreover, the key weakness of
SVMs—the black box model representation—is less critical for image processing. If an
SVM can differentiate a cat from a dog, it does not matter much how it is doing so.

In this section, we will develop a model similar to those used at the core of the
Optical Character Recognition (OCR) software often bundled with desktop
document scanners. The purpose of such software is to process paper-based
documents by converting printed or handwritten text into an electronic form to be
saved in a database. Of course, this is a difficult problem due to the many variants
in handwritten style and printed fonts. Even so, software users expect perfection,
as errors or typos can result in embarrassing or costly mistakes in a business
environment. Let's see whether our SVM is up to the task.



Data collection:

When OCR software first processes a document, it divides the paper into a
matrix such that each cell in the grid contains a single glyph, which is just a term
referring to a letter, symbol, or number. Next, for each cell, the software will attempt
to match the glyph to a set of all characters it recognizes. Finally, the individual
characters would be combined back together into words, which optionally could be
spell-checked against a dictionary in the document's language.

In this exercise, we'll assume that we have already developed the algorithm to
partition the document into rectangular regions each consisting of a single character.
We will also assume the document contains only alphabetic characters in English.
Therefore, we'll simulate a process that involves matching glyphs to one of the 26
letters, A through Z.

To this end, we'll use a dataset donated to the UCI Machine Learning Data
Repository ( http://archive.ics.uci.edu/ml ) by W. Frey and D. J. Slate. The
dataset contains 20,000 examples of 26 English alphabet capital letters as printed
using 20 different randomly reshaped and distorted black and white fonts.



Data Attributes:

According to the documentation provided by Frey and Slate, when the glyphs are
scanned into the computer, they are converted into pixels and 16 statistical attributes
are recorded.

The attributes measure such characteristics as the horizontal and vertical
dimensions of the glyph, the proportion of black (versus white) pixels, and the
average horizontal and vertical position of the pixels. Presumably, differences in the
concentration of black pixels across various areas of the box should provide a way to
differentiate among the 26 letters of the alphabet.
